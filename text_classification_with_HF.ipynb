{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.9"
    },
    "colab": {
      "name": "text_classification_with_HF.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/alexlimatds/PyTorch-examples/blob/main/text_classification_with_HF.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sZcSW6EN-VWN"
      },
      "source": [
        "## Text classification with Hugging Face\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cUTW2eVhhPPM"
      },
      "source": [
        "### Basic example of sentiment detection\n",
        "\n",
        "This example uses a pretrained model to perform sentiment classification.\n",
        "\n",
        "Source: https://huggingface.co/transformers/quicktour.html"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "dOUL3OMpBhzh",
        "outputId": "9ba90dbd-d08f-4951-8610-37ecd4c5de5f"
      },
      "source": [
        "!pip install transformers"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: transformers in /usr/local/lib/python3.6/dist-packages (4.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.6/dist-packages (from transformers) (20.8)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers) (2019.12.20)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers) (3.0.12)\n",
            "Requirement already satisfied: tokenizers==0.9.4 in /usr/local/lib/python3.6/dist-packages (from transformers) (0.9.4)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers) (4.41.1)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers) (0.8)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers) (1.19.4)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers) (0.0.43)\n",
            "Requirement already satisfied: pyparsing>=2.0.2 in /usr/local/lib/python3.6/dist-packages (from packaging->transformers) (2.4.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2020.12.5)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers) (3.0.4)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.0.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (7.1.2)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ahlx-nzz-VWY"
      },
      "source": [
        "Downloading a pipeline which encapsulates a tokenizer and a pretrained model. The pipeline use the tokenizer to preprocess the text. The pipeline also performs post-process."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9VRiwQbL-VWa"
      },
      "source": [
        "from transformers import pipeline\n",
        "\n",
        "# Loading a pipeline with pretrained model and its tokenizer\n",
        "classifier = pipeline('sentiment-analysis')"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oCyDwjiG-VWc",
        "outputId": "c8cf12e2-5b06-40e9-f577-a72e2d5200ae"
      },
      "source": [
        "results = classifier(\n",
        "    [\"We are very happy to show you the ðŸ¤— Transformers library.\", \n",
        "     \"We hope you don't hate it.\", \n",
        "     \"This movie sucks!\"])\n",
        "\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label: POSITIVE, with score: 0.9998\n",
            "label: NEGATIVE, with score: 0.5309\n",
            "label: NEGATIVE, with score: 0.9992\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "aEY5qcMw-VWg"
      },
      "source": [
        "We are able to get the tokenizer and the model from the pipeline object."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ew9qtxMW-VWh",
        "outputId": "da3a231e-b866-42fd-85a4-d1ed44a48bb0"
      },
      "source": [
        "tokenizer = classifier.tokenizer\n",
        "print(tokenizer(\"Saving Private Ryan is a great movie.\"))\n",
        "\n",
        "model = classifier.model\n",
        "print(\"Model: \", type(model).__name__)"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'input_ids': [101, 7494, 2797, 4575, 2003, 1037, 2307, 3185, 1012, 102], 'attention_mask': [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]}\n",
            "Model:  DistilBertForSequenceClassification\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2l7hwX9t-VWj"
      },
      "source": [
        "### Fine tuning\n",
        "\n",
        "We can perform fine tuning on a pretrained model. It's possible to change the number of labels and maintain the pretrained weights in the first layers.\n",
        "\n",
        "Let's download a pretrained model and its tokenizer. The model will predict the star number (1 to 5) of a product review."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "psuBvPtF-VWk",
        "outputId": "b8999c8d-e1c7-4cdf-e857-622f94af78b7"
      },
      "source": [
        "from transformers import DistilBertTokenizerFast, DistilBertForSequenceClassification\n",
        "\n",
        "model_name = \"distilbert-base-uncased\"\n",
        "model = DistilBertForSequenceClassification.from_pretrained(model_name, num_labels=5) # 5 classes - one for each star\n",
        "tokenizer = DistilBertTokenizerFast.from_pretrained(model_name)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertForSequenceClassification: ['vocab_transform.weight', 'vocab_transform.bias', 'vocab_layer_norm.weight', 'vocab_layer_norm.bias', 'vocab_projector.weight', 'vocab_projector.bias']\n",
            "- This IS expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing DistilBertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of DistilBertForSequenceClassification were not initialized from the model checkpoint at distilbert-base-uncased and are newly initialized: ['pre_classifier.weight', 'pre_classifier.bias', 'classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5jFCQjWMjLtf"
      },
      "source": [
        "Installing Huggingface Dataset library."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1evKzcxoCQ5j",
        "outputId": "de8da99f-d3f2-43b7-b7fe-77526de62710"
      },
      "source": [
        "!pip install datasets"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: datasets in /usr/local/lib/python3.6/dist-packages (1.1.3)\n",
            "Requirement already satisfied: dill in /usr/local/lib/python3.6/dist-packages (from datasets) (0.3.3)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.6/dist-packages (from datasets) (1.19.4)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from datasets) (0.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.23.0)\n",
            "Requirement already satisfied: tqdm<4.50.0,>=4.27 in /usr/local/lib/python3.6/dist-packages (from datasets) (4.41.1)\n",
            "Requirement already satisfied: xxhash in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.6/dist-packages (from datasets) (1.1.5)\n",
            "Requirement already satisfied: multiprocess in /usr/local/lib/python3.6/dist-packages (from datasets) (0.70.11.1)\n",
            "Requirement already satisfied: pyarrow>=0.17.1 in /usr/local/lib/python3.6/dist-packages (from datasets) (2.0.0)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2020.12.5)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests>=2.19.0->datasets) (2.10)\n",
            "Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2.8.1)\n",
            "Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.6/dist-packages (from pandas->datasets) (2018.9)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.6/dist-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "pnjCut9OjbPi"
      },
      "source": [
        "Loading the The Multilingual Amazon Reviews Corpus dataset. Details about it can be found at https://huggingface.co/datasets/"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-rFnpqI7-VWo",
        "outputId": "02dc7701-603a-43c5-a1aa-6bdecd5cb5fd"
      },
      "source": [
        "from datasets import load_dataset\n",
        "\n",
        "dataset_full = load_dataset('amazon_reviews_multi', 'en')"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reusing dataset amazon_reviews_multi (/root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9EiIEc6PJOQI",
        "outputId": "419d428c-5f08-4320-9a4d-e2470435078d"
      },
      "source": [
        "print('Number of data instances: ', len(dataset_full['train']))\n",
        "dataset_full['train'][0]"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Number of data instances:  200000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'language': 'en',\n",
              " 'product_category': 'furniture',\n",
              " 'product_id': 'product_en_0740675',\n",
              " 'review_body': \"Arrived broken. Manufacturer defect. Two of the legs of the base were not completely formed, so there was no way to insert the casters. I unpackaged the entire chair and hardware before noticing this. So, I'll spend twice the amount of time boxing up the whole useless thing and send it back with a 1-star review of part of a chair I never got to sit in. I will go so far as to include a picture of what their injection molding and quality assurance process missed though. I will be hesitant to buy again. It makes me wonder if there aren't missing structures and supports that don't impede the assembly process.\",\n",
              " 'review_id': 'en_0964290',\n",
              " 'review_title': \"I'll spend twice the amount of time boxing up the whole useless thing and send it back with a 1-star review ...\",\n",
              " 'reviewer_id': 'reviewer_en_0342986',\n",
              " 'stars': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "et3IcTAA0j8c"
      },
      "source": [
        "def count_by_stars(ds):\n",
        "  stars = [1, 2, 3, 4, 5]\n",
        "  for l in stars:\n",
        "    print(\"Instances for {} stars: {}\".format(l, ds.filter(lambda record: record['stars'] == l).num_rows))"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGv2FrwOOU3a",
        "outputId": "d0aaf0dc-caae-4d87-c1ae-816c995050f5"
      },
      "source": [
        "dataset_train = dataset_full['train']\n",
        "dataset_val = dataset_full['validation']\n",
        "\n",
        "print(\"Train dataset:\")\n",
        "count_by_stars(dataset_train)\n",
        "print(\"Validation dataset:\")\n",
        "count_by_stars(dataset_val)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-74e409560f0bf636.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-714f51de2678f108.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-5c4e8200da3f95e7.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-1741aa285ecc9000.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-5daa13abe1a9fb64.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-b13c562dc5150afd.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-39583fd99706f217.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-030027244742e2b5.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-90affffb787fe5da.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-724cca5ab58d58ec.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Train dataset:\n",
            "Instances for 1 stars: 40000\n",
            "Instances for 2 stars: 40000\n",
            "Instances for 3 stars: 40000\n",
            "Instances for 4 stars: 40000\n",
            "Instances for 5 stars: 40000\n",
            "Validation dataset:\n",
            "Instances for 1 stars: 1000\n",
            "Instances for 2 stars: 1000\n",
            "Instances for 3 stars: 1000\n",
            "Instances for 4 stars: 1000\n",
            "Instances for 5 stars: 1000\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EB5Q1ioy-VWp",
        "outputId": "43e2ae27-33d8-4477-c3f4-0c9a9cddb120"
      },
      "source": [
        "print('Features: ', dataset_train.features)\n",
        "dataset_train[0]"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Features:  {'review_id': Value(dtype='string', id=None), 'product_id': Value(dtype='string', id=None), 'reviewer_id': Value(dtype='string', id=None), 'stars': Value(dtype='int32', id=None), 'review_body': Value(dtype='string', id=None), 'review_title': Value(dtype='string', id=None), 'language': Value(dtype='string', id=None), 'product_category': Value(dtype='string', id=None)}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'language': 'en',\n",
              " 'product_category': 'furniture',\n",
              " 'product_id': 'product_en_0740675',\n",
              " 'review_body': \"Arrived broken. Manufacturer defect. Two of the legs of the base were not completely formed, so there was no way to insert the casters. I unpackaged the entire chair and hardware before noticing this. So, I'll spend twice the amount of time boxing up the whole useless thing and send it back with a 1-star review of part of a chair I never got to sit in. I will go so far as to include a picture of what their injection molding and quality assurance process missed though. I will be hesitant to buy again. It makes me wonder if there aren't missing structures and supports that don't impede the assembly process.\",\n",
              " 'review_id': 'en_0964290',\n",
              " 'review_title': \"I'll spend twice the amount of time boxing up the whole useless thing and send it back with a 1-star review ...\",\n",
              " 'reviewer_id': 'reviewer_en_0342986',\n",
              " 'stars': 1}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6dCgKpBgfeg8"
      },
      "source": [
        "In the following function, we use the tokenizer to preprocess the review text. The `map` method from the dataset object inserts the generated fields (`attention_mask` and `input_ids`) into the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zz6PXag9-VWs"
      },
      "source": [
        "def preprocess(ds):\n",
        "  return ds.map(\n",
        "      lambda batch: tokenizer(\n",
        "          batch[\"review_body\"], \n",
        "          truncation=True, \n",
        "          padding='max_length', \n",
        "          max_length=60), \n",
        "      batched=True)\n"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LHR-7MLoS_5H",
        "outputId": "a20be554-2531-4d1f-ceec-e5bfe466d917"
      },
      "source": [
        "dataset_train = preprocess(dataset_train)\n",
        "dataset_val = preprocess(dataset_val)\n",
        "\n",
        "# In the output, check the inserted fields\n",
        "dataset_train.features"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-a28e4f3885e3776e.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-a9c680c02a8d5621.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
              " 'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
              " 'language': Value(dtype='string', id=None),\n",
              " 'product_category': Value(dtype='string', id=None),\n",
              " 'product_id': Value(dtype='string', id=None),\n",
              " 'review_body': Value(dtype='string', id=None),\n",
              " 'review_id': Value(dtype='string', id=None),\n",
              " 'review_title': Value(dtype='string', id=None),\n",
              " 'reviewer_id': Value(dtype='string', id=None),\n",
              " 'stars': Value(dtype='int32', id=None)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CpXfXgHLHhLn"
      },
      "source": [
        "# The labels must be numbered from 0 to N-1 (N stands for the number of classes)\n",
        "def adjust_labels(data_instance):\n",
        "  data_instance['labels'] = data_instance['stars'] - 1  # The model requires 'labels' as the name of the column contaning the labels\n",
        "  return data_instance"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "M1VlwpqURYa7",
        "outputId": "ad8cf640-7517-43de-8b2b-078fbb204a5e"
      },
      "source": [
        "dataset_train = dataset_train.map(adjust_labels)\n",
        "dataset_val = dataset_val.map(adjust_labels)\n",
        "\n",
        "dataset_val.features"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-181535b273002c6b.arrow\n",
            "Loading cached processed dataset at /root/.cache/huggingface/datasets/amazon_reviews_multi/en/1.0.0/f3357bd271e187385a38574fe31b8fb10055303f67fa9fce55e84d08c4870efd/cache-c6c402349c94a3d2.arrow\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'attention_mask': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
              " 'input_ids': Sequence(feature=Value(dtype='int64', id=None), length=-1, id=None),\n",
              " 'labels': Value(dtype='int64', id=None),\n",
              " 'language': Value(dtype='string', id=None),\n",
              " 'product_category': Value(dtype='string', id=None),\n",
              " 'product_id': Value(dtype='string', id=None),\n",
              " 'review_body': Value(dtype='string', id=None),\n",
              " 'review_id': Value(dtype='string', id=None),\n",
              " 'review_title': Value(dtype='string', id=None),\n",
              " 'reviewer_id': Value(dtype='string', id=None),\n",
              " 'stars': Value(dtype='int32', id=None)}"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zWicRd3SlhK3"
      },
      "source": [
        "We use the `set_format` method to indicates which columns will be used as input during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q3mUNOxpPXzC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a0bc3635-6eb7-447c-be5a-a128244e4636"
      },
      "source": [
        "dataset_train.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "dataset_val.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "print({key: val.shape for key, val in dataset_train[0].items()})"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{'attention_mask': torch.Size([60]), 'input_ids': torch.Size([60]), 'labels': torch.Size([])}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/datasets/arrow_dataset.py:850: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:141.)\n",
            "  return torch.tensor(x, **format_kwargs)\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vPB0b_gimPTq"
      },
      "source": [
        "Let's perform training. We'll use the Huggingface facilities but it is possible to apply the PyTorch way."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9CbRN_dY-VWu"
      },
      "source": [
        "from transformers import Trainer, TrainingArguments\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir='./results',          # output directory\n",
        "    num_train_epochs=1,              # total # of training epochs\n",
        "    per_device_train_batch_size=32,  # batch size per device during training\n",
        "    per_device_eval_batch_size=64,   # batch size for evaluation\n",
        "    warmup_steps=500,                # number of warmup steps for learning rate scheduler\n",
        "    weight_decay=0.01,               # strength of weight decay\n",
        "    logging_dir='./logs',            # directory for storing logs\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,                         # the instantiated Transformers model to be trained\n",
        "    args=training_args,                  # training arguments, defined above\n",
        "    train_dataset=dataset_train,         # training dataset\n",
        "    eval_dataset=dataset_val             # evaluation dataset\n",
        ")"
      ],
      "execution_count": 17,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 469
        },
        "id": "BMxyAfD4ICBD",
        "outputId": "5627052f-c86c-43ef-8eb0-8bc5df0d241e"
      },
      "source": [
        "trainer.train()"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "        <style>\n",
              "            /* Turns off some styling */\n",
              "            progress {\n",
              "                /* gets rid of default border in Firefox and Opera. */\n",
              "                border: none;\n",
              "                /* Needs to be in here for Safari polyfill so background images work as expected. */\n",
              "                background-size: auto;\n",
              "            }\n",
              "        </style>\n",
              "      \n",
              "      <progress value='6250' max='6250' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [6250/6250 23:36, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: left;\">\n",
              "      <th>Step</th>\n",
              "      <th>Training Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>500</td>\n",
              "      <td>1.254666</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1000</td>\n",
              "      <td>1.073080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>1500</td>\n",
              "      <td>1.048080</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2000</td>\n",
              "      <td>1.019723</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2500</td>\n",
              "      <td>0.998885</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3000</td>\n",
              "      <td>0.995968</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3500</td>\n",
              "      <td>0.984322</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4000</td>\n",
              "      <td>0.976184</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4500</td>\n",
              "      <td>0.963402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5000</td>\n",
              "      <td>0.968773</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5500</td>\n",
              "      <td>0.966874</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6000</td>\n",
              "      <td>0.951705</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "TrainOutput(global_step=6250, training_loss=1.0137391796875)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UaA8KzjuILyZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f6a42a74-2765-4275-9957-ef811dd50788"
      },
      "source": [
        "from transformers import TextClassificationPipeline\n",
        "\n",
        "sequences = [\n",
        "    \"We are very happy to show you the Transformers library.\", \n",
        "    \"We hope you don't hate it.\", \n",
        "    \"This movie sucks!\", \n",
        "    \"The device works fine and its battery longs for hours. I recommend it.\", \n",
        "    \"The movie has good and bad moments.\", \n",
        "    \"The main features work well but some details can improve such the eject button\"]\n",
        "\n",
        "classifier2 = TextClassificationPipeline(\n",
        "    model=model, \n",
        "    tokenizer=tokenizer, \n",
        "    device=model.device.index)  # put the pipeline on the same model's device\n",
        "\n",
        "results = classifier2(sequences)\n",
        "for result in results:\n",
        "    print(f\"label: {result['label']}, with score: {round(result['score'], 4)}\")"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "label: LABEL_4, with score: 0.644\n",
            "label: LABEL_2, with score: 0.3281\n",
            "label: LABEL_0, with score: 0.95\n",
            "label: LABEL_4, with score: 0.5817\n",
            "label: LABEL_2, with score: 0.4097\n",
            "label: LABEL_3, with score: 0.6181\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PEaYW6E8puu-"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}